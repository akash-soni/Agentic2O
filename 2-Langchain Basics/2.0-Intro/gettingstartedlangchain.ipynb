{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GenAIAPPwithOPENAI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000012E2DD8E6C0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000012E2F603E90> root_client=<openai.OpenAI object at 0x0000012E2DFD1610> root_async_client=<openai.AsyncOpenAI object at 0x0000012E2F22B890> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems designed with characteristics akin to agency, enabling them to operate autonomously, make independent decisions, and take actions toward achieving specific objectives without constant human oversight. The concept of agency in this context draws parallels to human or animal agency, where an entity can perceive its environment, set goals, plan strategies, and execute actions to fulfill those goals.\\n\\n### Key Characteristics of Agentic AI:\\n\\n1. **Autonomy:** Agentic AI systems can function independently, making decisions without needing real-time human intervention. This autonomy allows them to handle tasks in dynamic environments where pre-programmed instructions may be insufficient.\\n\\n2. **Perception:** These AI systems are equipped with sensors or data inputs that allow them to perceive and interpret their surroundings. This perception is crucial for informed decision-making and adaptive behavior.\\n\\n3. **Goal-Oriented Behavior:** Agentic AI operates based on defined objectives or goals. They can prioritize tasks, plan sequences of actions, and adjust strategies to efficiently achieve their targets.\\n\\n4. **Learning and Adaptation:** Such AI systems often incorporate machine learning algorithms that enable them to learn from experiences, adapt to new situations, and improve their performance over time.\\n\\n5. **Decision-Making:** Agentic AI possesses the capability to evaluate options, assess potential outcomes, and make choices that align with their goals. This involves reasoning, problem-solving, and sometimes even ethical considerations.\\n\\n### Applications of Agentic AI:\\n\\n- **Autonomous Vehicles:** Self-driving cars exemplify agentic AI by navigating roads, making real-time decisions to ensure safety, and optimizing routes without human input.\\n  \\n- **Robotics:** Service robots in industries like manufacturing, healthcare, and logistics use agentic AI to perform tasks autonomously, adapt to changing environments, and collaborate with humans and other robots.\\n  \\n- **Virtual Assistants:** Advanced virtual assistants can manage schedules, control smart home devices, and perform complex tasks by understanding user preferences and making proactive decisions.\\n  \\n- **Healthcare:** AI systems can autonomously monitor patient data, predict health issues, and recommend treatments, improving efficiency and outcomes in medical settings.\\n\\n### Considerations and Challenges:\\n\\nWhile Agentic AI offers numerous advantages in terms of efficiency and capability, it also presents challenges:\\n\\n- **Ethical Concerns:** Autonomous decision-making raises questions about accountability, especially in scenarios where AI actions may have significant consequences.\\n  \\n- **Safety and Reliability:** Ensuring that agentic AI systems operate safely in all possible scenarios is critical, particularly in applications like autonomous driving or healthcare.\\n  \\n- **Transparency:** Understanding how AI systems make decisions is essential for trust and verification, necessitating transparent algorithms and processes.\\n  \\n- **Regulation:** Developing appropriate regulatory frameworks to govern the deployment and behavior of agentic AI is an ongoing challenge.\\n\\n### Future Outlook:\\n\\nThe development of Agentic AI is a rapidly evolving field, driven by advancements in machine learning, robotics, and cognitive computing. As these systems become more sophisticated, they hold the promise of transforming industries, enhancing productivity, and enabling new capabilities. However, balancing innovation with ethical considerations and robust safeguards will be crucial to ensuring that Agentic AI benefits society as a whole.\\n\\n---\\n\\n*Note:* The term \"Agentic AI\" can sometimes be used interchangeably with \"autonomous AI\" or \"AI agents,\" though nuances may exist depending on the specific context or framework being referenced. It\\'s essential to consider the particular definitions and applications within different AI disciplines or discussions.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1023, 'prompt_tokens': 13, 'total_tokens': 1036, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_7989eaacf6', 'id': 'chatcmpl-BaywDE4XjamJDTeTGer5M6Wfsm765', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b378ac39-cc16-4147-a6cd-8a4291d6b04a-0' usage_metadata={'input_tokens': 13, 'output_tokens': 1023, 'total_tokens': 1036, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems designed with characteristics akin to agency, enabling them to operate autonomously, make independent decisions, and take actions toward achieving specific objectives without constant human oversight. The concept of agency in this context draws parallels to human or animal agency, where an entity can perceive its environment, set goals, plan strategies, and execute actions to fulfill those goals.\n",
      "\n",
      "### Key Characteristics of Agentic AI:\n",
      "\n",
      "1. **Autonomy:** Agentic AI systems can function independently, making decisions without needing real-time human intervention. This autonomy allows them to handle tasks in dynamic environments where pre-programmed instructions may be insufficient.\n",
      "\n",
      "2. **Perception:** These AI systems are equipped with sensors or data inputs that allow them to perceive and interpret their surroundings. This perception is crucial for informed decision-making and adaptive behavior.\n",
      "\n",
      "3. **Goal-Oriented Behavior:** Agentic AI operates based on defined objectives or goals. They can prioritize tasks, plan sequences of actions, and adjust strategies to efficiently achieve their targets.\n",
      "\n",
      "4. **Learning and Adaptation:** Such AI systems often incorporate machine learning algorithms that enable them to learn from experiences, adapt to new situations, and improve their performance over time.\n",
      "\n",
      "5. **Decision-Making:** Agentic AI possesses the capability to evaluate options, assess potential outcomes, and make choices that align with their goals. This involves reasoning, problem-solving, and sometimes even ethical considerations.\n",
      "\n",
      "### Applications of Agentic AI:\n",
      "\n",
      "- **Autonomous Vehicles:** Self-driving cars exemplify agentic AI by navigating roads, making real-time decisions to ensure safety, and optimizing routes without human input.\n",
      "  \n",
      "- **Robotics:** Service robots in industries like manufacturing, healthcare, and logistics use agentic AI to perform tasks autonomously, adapt to changing environments, and collaborate with humans and other robots.\n",
      "  \n",
      "- **Virtual Assistants:** Advanced virtual assistants can manage schedules, control smart home devices, and perform complex tasks by understanding user preferences and making proactive decisions.\n",
      "  \n",
      "- **Healthcare:** AI systems can autonomously monitor patient data, predict health issues, and recommend treatments, improving efficiency and outcomes in medical settings.\n",
      "\n",
      "### Considerations and Challenges:\n",
      "\n",
      "While Agentic AI offers numerous advantages in terms of efficiency and capability, it also presents challenges:\n",
      "\n",
      "- **Ethical Concerns:** Autonomous decision-making raises questions about accountability, especially in scenarios where AI actions may have significant consequences.\n",
      "  \n",
      "- **Safety and Reliability:** Ensuring that agentic AI systems operate safely in all possible scenarios is critical, particularly in applications like autonomous driving or healthcare.\n",
      "  \n",
      "- **Transparency:** Understanding how AI systems make decisions is essential for trust and verification, necessitating transparent algorithms and processes.\n",
      "  \n",
      "- **Regulation:** Developing appropriate regulatory frameworks to govern the deployment and behavior of agentic AI is an ongoing challenge.\n",
      "\n",
      "### Future Outlook:\n",
      "\n",
      "The development of Agentic AI is a rapidly evolving field, driven by advancements in machine learning, robotics, and cognitive computing. As these systems become more sophisticated, they hold the promise of transforming industries, enhancing productivity, and enabling new capabilities. However, balancing innovation with ethical considerations and robust safeguards will be crucial to ensuring that Agentic AI benefits society as a whole.\n",
      "\n",
      "---\n",
      "\n",
      "*Note:* The term \"Agentic AI\" can sometimes be used interchangeably with \"autonomous AI\" or \"AI agents,\" though nuances may exist depending on the specific context or framework being referenced. It's essential to consider the particular definitions and applications within different AI disciplines or discussions.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"Hi My name is Krish.\" I should start by greeting them back. Let me make sure to use their name so it feels personal. Maybe something like \"Hi Krish! How can I assist you today?\" That\\'s friendly and opens the conversation for them to explain what they need help with. I should keep it simple and not overcomplicate. Let me check if there\\'s any other context I need to consider, but since this is the first message, just a straightforward response should be best.\\n</think>\\n\\nHi Krish! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 15, 'total_tokens': 154, 'completion_time': 0.339527311, 'prompt_time': 0.00290619, 'queue_time': 0.243538789, 'total_time': 0.342433501}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--51048f61-fa88-4302-b416-aeedcc9f5f32-0', usage_metadata={'input_tokens': 15, 'output_tokens': 139, 'total_tokens': 154})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"Hi My name is Krish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000012E2F7767E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000012E3B2DD8E0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000012E2F7767E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000012E3B2DD8E0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I can definitely tell you about Langsmith!\n",
      "\n",
      "Langsmith is a powerful and innovative open-source platform designed to simplify the development and deployment of language models. \n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "**Core Features:**\n",
      "\n",
      "* **Model Management:** Langsmith excels at managing various language models, allowing you to easily switch between them, track their performance, and experiment with different architectures.\n",
      "* **Prompt Engineering:** It provides a user-friendly interface and tools to craft and refine prompts, a crucial aspect of getting the best results from language models.\n",
      "* **Pipeline Creation:**  Langsmith lets you build complex workflows by chaining together different language model tasks, such as text generation, summarization, translation, and question answering.\n",
      "* **Data Integration:**  It seamlessly integrates with various data sources, enabling you to train and fine-tune your models on your own datasets.\n",
      "* **Deployment:** Langsmith simplifies the process of deploying your trained models as APIs or web applications, making them accessible for use in real-world applications.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Accessibility:** Being open-source, Langsmith democratizes access to powerful language model capabilities, allowing developers and researchers to experiment and innovate without significant financial barriers.\n",
      "* **Flexibility:** Its modular design and extensibility enable you to customize the platform to your specific needs and integrate it with other tools and services.\n",
      "* **Collaboration:** The open-source nature fosters a community of developers who contribute to its growth, share knowledge, and collaborate on projects.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "* **Chatbots and Conversational AI:**  Build intelligent chatbots for customer service, education, or entertainment.\n",
      "* **Content Generation:** Create articles, blog posts, marketing copy, and other types of content automatically.\n",
      "* **Text Summarization and Analysis:**  Extract key insights and summarize large volumes of text.\n",
      "* **Code Generation and Assistance:**  Generate code snippets, assist with code completion, and improve coding efficiency.\n",
      "* **Research and Development:**  Explore new language model architectures, experiment with different training techniques, and advance the field of AI.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "The Langsmith team provides comprehensive documentation and tutorials to help you get started. You can find more information on their website and GitHub repository.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or anything else related to AI!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a large language model, I don't have access to real-time information or specific details about tools and platforms like \"Langsmith.\" My knowledge is based on the dataset I was trained on, which might not include information about recently developed tools.\n",
      "\n",
      "However, based on the name, \"Langsmith\" likely refers to a tool or platform related to language modeling. \n",
      "\n",
      "**Here are some general possibilities about what \"Langsmith\" could be:**\n",
      "\n",
      "* **A framework for building language models:** It could be a platform that provides tools and resources for developers to create their own language models.\n",
      "* **A platform for fine-tuning existing language models:** It might offer a way to specialize pre-trained language models for specific tasks or domains.\n",
      "* **A tool for interacting with and experimenting with language models:** It could be a user-friendly interface for testing and exploring the capabilities of different language models.\n",
      "\n",
      "**To find more specific information about \"Langsmith,\" I recommend:**\n",
      "\n",
      "* **Searching online:** Use a search engine like Google to look for information about \"Langsmith.\"\n",
      "* **Checking AI communities and forums:** Websites like Reddit or specialized AI forums might have discussions about this tool.\n",
      "* **Looking at AI-related news and blogs:** Tech news websites often report on new AI tools and platforms.\n",
      "\n",
      "\n",
      "Good luck in your search!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building, deploying, and managing AI assistants.', 'features': ['Customizable AI assistants', 'Easy-to-use interface', 'Integration with popular tools', 'Open-source and customizable', 'Community-driven development'], 'use_cases': ['Customer service', 'Education', 'Content creation', 'Personal productivity', 'Research'], 'website': 'https://www.langsmith.com/'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Langsmith is an open-source framework developed by the team at Weights & Biases (wandb) that aims to simplify the process of fine-tuning large language models (LLMs). \\n\\n  Here are some key features and benefits of Langsmith:\\n\\n  * **Simplified Fine-Tuning:** Langsmith provides a user-friendly interface and streamlined workflow for fine-tuning LLMs, making it accessible to a wider range of developers and researchers.\\n\\n  * **Efficient Training:** It leverages techniques like parameter-efficient fine-tuning and knowledge distillation to optimize the training process, reducing resource requirements and time.\\n\\n  * **Hardware Agnostic:** Langsmith supports various hardware platforms, including GPUs, TPUs, and CPUs, allowing users to choose the most suitable setup for their needs.\\n\\n  * **Experiment Tracking and Management:** Integration with Weights & Biases enables robust experiment tracking, logging, and visualization, facilitating reproducibility and model comparison.\\n\\n  * **Community-Driven Development:** As an open-source project, Langsmith benefits from contributions and improvements from the wider AI community, fostering collaboration and innovation.\\n\\n  Overall, Langsmith empowers developers and researchers to easily fine-tune LLMs for specific tasks and applications, unlocking the potential of these powerful models.', 'source': 'My knowledge about Langsmith comes from publicly available information and documentation.'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n  <name>Langsmith</name>\\n  <description>Langsmith is an open-source platform for building and deploying large language models (LLMs). It focuses on making LLMs more accessible and customizable for developers.</description>\\n  <features>\\n    <feature>Modular and extensible architecture</feature>\\n    <feature>Support for various LLM architectures</feature>\\n    <feature>Tools for fine-tuning and evaluating LLMs</feature>\\n    <feature>Integration with popular machine learning frameworks</feature>\\n  </features>\\n  <website>https://github.com/facebookresearch/langsmith</website>\\n</response> \\n``` \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 195, 'total_tokens': 347, 'completion_time': 0.276363636, 'prompt_time': 0.012164595, 'queue_time': 0.24441523199999998, 'total_time': 0.288528231}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--a73fd174-006f-44b2-905e-61ed985f2fdc-0' usage_metadata={'input_tokens': 195, 'output_tokens': 152, 'total_tokens': 347}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55c9e2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```xml\n",
      "<response>\n",
      "  <name>Langsmith</name>\n",
      "  <description>Langsmith is an open-source platform for building and deploying large language models (LLMs). It focuses on making LLMs more accessible and customizable for developers.</description>\n",
      "  <features>\n",
      "    <feature>Modular and extensible architecture</feature>\n",
      "    <feature>Support for various LLM architectures</feature>\n",
      "    <feature>Tools for fine-tuning and evaluating LLMs</feature>\n",
      "    <feature>Integration with popular machine learning frameworks</feature>\n",
      "  </features>\n",
      "  <website>https://github.com/facebookresearch/langsmith</website>\n",
      "</response> \n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a structured way to chain together different components, such as LLMs, prompts, memory, and other tools, to build more sophisticated and versatile AI applications.</answer></response> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why couldn't the bicycle stand up by itself? Because it was two tired!\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Sully</movie>\n",
      "<movie>Toy Story</movie>\n",
      "<movie>Sleepless in Seattle</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03277d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Step 1: Define the output schema\n",
    "class ProductInfo(BaseModel):\n",
    "    product_name: str = Field(..., description=\"Name of the product\")\n",
    "    product_details: str = Field(..., description=\"Short description of the product\")\n",
    "    tentative_price_usd: int = Field(..., description=\"Tentative price in USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ed67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the output parser\n",
    "parser = PydanticOutputParser(pydantic_object=ProductInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721cb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template =\"\"\"You are a helpful product assistant. When given a product name, respond with:\n",
    "- product name\n",
    "- short product details\n",
    "- a tentative price in USD (as an integer).\\n\n",
    "\n",
    "{format_instructions}\\n\n",
    "\n",
    "Product: {product_query}\n",
    "\"\"\",\n",
    "    input_variables=[\"product_query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5012f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize the LLM (using GPT-3.5 or GPT-4 via OpenAI)\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a52743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "product = \"Apple iPhone 15 Pro\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ec9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218eb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"product_query\": product})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df39d5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_name='Apple iPhone 15 Pro' product_details='The latest flagship smartphone from Apple with advanced features and technology.' tentative_price_usd=1200\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999f98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
